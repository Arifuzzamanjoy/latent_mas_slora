{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0f91ca",
   "metadata": {},
   "source": [
    "# LatentMAS RunPod API - Complete Usage Guide\n",
    "\n",
    "This notebook demonstrates all features of the **LatentMAS + S-LoRA** multi-agent system via the RunPod serverless API.\n",
    "\n",
    "## Features Covered\n",
    "1. **Basic Chat & Inference** - Simple text queries with domain routing\n",
    "2. **Vision Language Model (VLM)** - Image + text queries\n",
    "3. **RAG (Retrieval Augmented Generation)** - Document injection\n",
    "4. **Conversation Continuity** - Session & conversation management\n",
    "5. **LoRA Adapter Selection** - Domain-specific fine-tuned models\n",
    "6. **Tool Execution** - Function calling capabilities\n",
    "7. **Metadata Queries** - List adapters, conversations, sessions\n",
    "\n",
    "## Architecture\n",
    "- **Base Model**: Qwen/Qwen2.5-VL-7B-Instruct (8B params, VLM)\n",
    "- **Pipeline**: Planner ‚Üí Critic (latent) ‚Üí Refiner (latent) ‚Üí Judger\n",
    "- **Adapters**: Medical, Math, Code, Reasoning LoRAs\n",
    "- **RAG**: Document retrieval with embeddings\n",
    "- **Persistence**: Session-based conversation memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661f7192",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "\n",
    "First, configure your RunPod API credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import time\n",
    "from typing import Dict, Any, Optional, List\n",
    "\n",
    "# ==============================================\n",
    "# RunPod Configuration\n",
    "# ==============================================\n",
    "API_KEY = \"rpa_VHF8RTJVI3H5XX\"  # Your RunPod API key\n",
    "ENDPOINT_ID = \"te7m0xova7z4rrXXX\"  # Your endpoint ID\n",
    "\n",
    "# Construct the API endpoint URL\n",
    "RUNPOD_API_URL = f\"https://api.runpod.ai/v2/{ENDPOINT_ID}/run\"\n",
    "\n",
    "# Headers for all requests\n",
    "HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\"\n",
    "}\n",
    "\n",
    "print(\"‚úÖ RunPod API configured\")\n",
    "print(f\"üì° Endpoint: {RUNPOD_API_URL}\")\n",
    "print(f\"üîë API Key: {API_KEY[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46f743",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Utility functions for making API requests and handling responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_latent_mas(\n",
    "    prompt: str,\n",
    "    image_url: Optional[str] = None,\n",
    "    image_base64: Optional[str] = None,\n",
    "    max_tokens: int = 800,\n",
    "    temperature: float = 0.7,\n",
    "    system_prompt: Optional[str] = None,\n",
    "    conversation_id: Optional[str] = None,\n",
    "    session_id: Optional[str] = None,\n",
    "    lora_adapter: Optional[str] = None,\n",
    "    lora_hf_path: Optional[str] = None,\n",
    "    rag_data: Optional[str] = None,\n",
    "    rag_documents: Optional[List[Dict]] = None,\n",
    "    no_default_data: bool = True,\n",
    "    enable_tools: bool = False,\n",
    "    model: str = \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    wait_for_completion: bool = True,\n",
    "    timeout: int = 300,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Call the LatentMAS RunPod endpoint with comprehensive parameters.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The query text (required)\n",
    "        image_url: URL of an image for VLM analysis\n",
    "        image_base64: Base64-encoded image data\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0.0-1.0)\n",
    "        system_prompt: Custom system prompt\n",
    "        conversation_id: ID to continue existing conversation\n",
    "        session_id: ID to group conversations\n",
    "        lora_adapter: LoRA adapter name from registry\n",
    "        lora_hf_path: Direct HuggingFace LoRA path\n",
    "        rag_data: URL or base64 encoded data for RAG\n",
    "        rag_documents: List of documents for RAG\n",
    "        no_default_data: Skip built-in RAG data\n",
    "        enable_tools: Enable tool execution\n",
    "        model: Model name to use\n",
    "        wait_for_completion: Wait for async job to complete\n",
    "        timeout: Timeout in seconds for completion\n",
    "    \n",
    "    Returns:\n",
    "        Response dictionary with results\n",
    "    \"\"\"\n",
    "    # Build request payload\n",
    "    payload = {\n",
    "        \"input\": {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"model\": model,\n",
    "            \"no_default_data\": no_default_data,\n",
    "            \"enable_tools\": enable_tools,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add optional parameters\n",
    "    if image_url:\n",
    "        payload[\"input\"][\"image_url\"] = image_url\n",
    "    if image_base64:\n",
    "        payload[\"input\"][\"image_base64\"] = image_base64\n",
    "    if system_prompt:\n",
    "        payload[\"input\"][\"system_prompt\"] = system_prompt\n",
    "    if conversation_id:\n",
    "        payload[\"input\"][\"conversation_id\"] = conversation_id\n",
    "    if session_id:\n",
    "        payload[\"input\"][\"session_id\"] = session_id\n",
    "    if lora_adapter:\n",
    "        payload[\"input\"][\"lora_adapter\"] = lora_adapter\n",
    "    if lora_hf_path:\n",
    "        payload[\"input\"][\"lora_hf_path\"] = lora_hf_path\n",
    "    if rag_data:\n",
    "        payload[\"input\"][\"rag_data\"] = rag_data\n",
    "    if rag_documents:\n",
    "        payload[\"input\"][\"rag_documents\"] = rag_documents\n",
    "    \n",
    "    # Submit job\n",
    "    response = requests.post(RUNPOD_API_URL, headers=HEADERS, json=payload)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    \n",
    "    if not wait_for_completion:\n",
    "        return result\n",
    "    \n",
    "    # Poll for completion\n",
    "    job_id = result.get(\"id\")\n",
    "    status_url = f\"https://api.runpod.ai/v2/{ENDPOINT_ID}/status/{job_id}\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        status_response = requests.get(status_url, headers=HEADERS)\n",
    "        status_response.raise_for_status()\n",
    "        status_data = status_response.json()\n",
    "        \n",
    "        status = status_data.get(\"status\")\n",
    "        \n",
    "        if status == \"COMPLETED\":\n",
    "            return status_data.get(\"output\", {})\n",
    "        elif status == \"FAILED\":\n",
    "            raise Exception(f\"Job failed: {status_data.get('error', 'Unknown error')}\")\n",
    "        \n",
    "        time.sleep(2)  # Poll every 2 seconds\n",
    "    \n",
    "    raise TimeoutError(f\"Job did not complete within {timeout} seconds\")\n",
    "\n",
    "\n",
    "def list_available_loras() -> Dict[str, Any]:\n",
    "    \"\"\"List all available LoRA adapters in the registry.\"\"\"\n",
    "    payload = {\n",
    "        \"input\": {\n",
    "            \"list_loras\": True\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(RUNPOD_API_URL, headers=HEADERS, json=payload)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    \n",
    "    # Handle async response\n",
    "    if \"id\" in result:\n",
    "        job_id = result[\"id\"]\n",
    "        status_url = f\"https://api.runpod.ai/v2/{ENDPOINT_ID}/status/{job_id}\"\n",
    "        \n",
    "        for _ in range(30):  # 60 seconds timeout\n",
    "            status_response = requests.get(status_url, headers=HEADERS)\n",
    "            status_response.raise_for_status()\n",
    "            status_data = status_response.json()\n",
    "            \n",
    "            if status_data.get(\"status\") == \"COMPLETED\":\n",
    "                return status_data.get(\"output\", {})\n",
    "            elif status_data.get(\"status\") == \"FAILED\":\n",
    "                raise Exception(f\"Failed to list LoRAs: {status_data.get('error')}\")\n",
    "            \n",
    "            time.sleep(2)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def list_conversations() -> Dict[str, Any]:\n",
    "    \"\"\"List all saved conversations and sessions.\"\"\"\n",
    "    payload = {\n",
    "        \"input\": {\n",
    "            \"list_conversations\": True\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(RUNPOD_API_URL, headers=HEADERS, json=payload)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    \n",
    "    # Handle async response\n",
    "    if \"id\" in result:\n",
    "        job_id = result[\"id\"]\n",
    "        status_url = f\"https://api.runpod.ai/v2/{ENDPOINT_ID}/status/{job_id}\"\n",
    "        \n",
    "        for _ in range(30):\n",
    "            status_response = requests.get(status_url, headers=HEADERS)\n",
    "            status_response.raise_for_status()\n",
    "            status_data = status_response.json()\n",
    "            \n",
    "            if status_data.get(\"status\") == \"COMPLETED\":\n",
    "                return status_data.get(\"output\", {})\n",
    "            elif status_data.get(\"status\") == \"FAILED\":\n",
    "                raise Exception(f\"Failed to list conversations: {status_data.get('error')}\")\n",
    "            \n",
    "            time.sleep(2)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def print_response(response: Dict[str, Any], show_full: bool = False):\n",
    "    \"\"\"Pretty print the API response.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üì• RESPONSE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if \"response\" in response:\n",
    "        print(f\"\\nüí¨ Answer:\\n{response['response']}\\n\")\n",
    "    \n",
    "    if \"error\" in response:\n",
    "        print(f\"\\n‚ùå Error: {response['error']}\\n\")\n",
    "        if \"traceback\" in response:\n",
    "            print(f\"Traceback:\\n{response['traceback']}\")\n",
    "        return\n",
    "    \n",
    "    # Metadata\n",
    "    print(\"üìä Metadata:\")\n",
    "    print(f\"  ‚Ä¢ Domain: {response.get('domain', 'N/A')} (confidence: {response.get('domain_confidence', 0):.2f})\")\n",
    "    print(f\"  ‚Ä¢ Model: {response.get('model', 'N/A')}\")\n",
    "    print(f\"  ‚Ä¢ VLM Mode: {'Yes' if response.get('vlm') else 'No'}\")\n",
    "    print(f\"  ‚Ä¢ Image Provided: {'Yes' if response.get('image_provided') else 'No'}\")\n",
    "    print(f\"  ‚Ä¢ RAG Enabled: {'Yes' if response.get('rag_enabled') else 'No'}\")\n",
    "    print(f\"  ‚Ä¢ Tools Enabled: {'Yes' if response.get('tools_enabled') else 'No'}\")\n",
    "    \n",
    "    if \"conversation_id\" in response:\n",
    "        print(f\"  ‚Ä¢ Conversation ID: {response['conversation_id'][:16]}...\")\n",
    "    if \"session_id\" in response:\n",
    "        print(f\"  ‚Ä¢ Session ID: {response['session_id'][:16]}...\")\n",
    "    \n",
    "    # LoRA info\n",
    "    if \"lora\" in response and response[\"lora\"].get(\"loaded\"):\n",
    "        lora_info = response[\"lora\"]\n",
    "        print(f\"  ‚Ä¢ LoRA Adapter: {lora_info.get('adapter', 'N/A')}\")\n",
    "        if \"hf_path\" in lora_info:\n",
    "            print(f\"    Path: {lora_info['hf_path']}\")\n",
    "    \n",
    "    if show_full:\n",
    "        print(\"\\nüîç Full Response:\")\n",
    "        print(json.dumps(response, indent=2))\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438d9bbc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Basic Chat & Inference\n",
    "\n",
    "Simple text queries with multi-agent reasoning and domain routing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618acad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple question\n",
    "print(\"üîπ Example 1: Simple Question\")\n",
    "response = call_latent_mas(\n",
    "    prompt=\"What is the capital of France?\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c188d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Complex reasoning (triggers multi-agent pipeline)\n",
    "print(\"üîπ Example 2: Complex Reasoning\")\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Explain the difference between supervised and unsupervised learning, with examples.\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be347ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Domain-specific query (medical)\n",
    "print(\"üîπ Example 3: Medical Domain\")\n",
    "response = call_latent_mas(\n",
    "    prompt=\"What are the symptoms and treatment for type 2 diabetes?\",\n",
    "    max_tokens=600,\n",
    "    temperature=0.6\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aec0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Math/Reasoning domain\n",
    "print(\"üîπ Example 4: Math Problem\")\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Solve: If a train travels at 80 km/h for 2.5 hours, then slows to 60 km/h for 1 hour, what is the total distance traveled?\",\n",
    "    max_tokens=400,\n",
    "    temperature=0.5\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Code generation\n",
    "print(\"üîπ Example 5: Code Domain\")\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Write a Python function to calculate the Fibonacci sequence up to n terms using dynamic programming.\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.5\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a1c95",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Vision Language Model (VLM)\n",
    "\n",
    "The base model supports image + text queries. You can provide images via URL or base64 encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81cd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Image analysis via URL\n",
    "print(\"üîπ Example 6: Image Analysis (URL)\")\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Describe what you see in this image in detail.\",\n",
    "    image_url=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "    max_tokens=400,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548bb49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: Medical image analysis (with medical LoRA)\n",
    "print(\"üîπ Example 7: Medical Image Analysis\")\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Analyze this chest X-ray image. Identify any abnormalities or key findings.\",\n",
    "    image_url=\"https://upload.wikimedia.org/wikipedia/commons/8/8c/Chest_Xray_PA_3-8-2010.png\",\n",
    "    lora_adapter=\"medical_vl\",  # Use medical vision LoRA\n",
    "    max_tokens=600,\n",
    "    temperature=0.6\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661041c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 8: Base64 image encoding (for local files)\n",
    "def encode_image_to_base64(image_path: str) -> str:\n",
    "    \"\"\"Encode a local image to base64 string.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Example usage (uncomment if you have a local image)\n",
    "# image_b64 = encode_image_to_base64(\"/path/to/your/image.jpg\")\n",
    "# response = call_latent_mas(\n",
    "#     prompt=\"What objects can you identify in this image?\",\n",
    "#     image_base64=image_b64,\n",
    "#     max_tokens=300\n",
    "# )\n",
    "# print_response(response)\n",
    "\n",
    "print(\"‚úÖ Image encoding function defined (uncomment to use with local images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e3e330",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. RAG (Retrieval Augmented Generation)\n",
    "\n",
    "Inject custom documents for context-aware responses. The system supports multiple RAG input methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c0693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 9: RAG with inline documents\n",
    "print(\"üîπ Example 9: RAG with Inline Documents\")\n",
    "\n",
    "# Define custom documents\n",
    "custom_docs = [\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "        Company Policy: Remote Work Guidelines\n",
    "        \n",
    "        All employees are eligible for remote work up to 3 days per week.\n",
    "        Employees must maintain core hours of 10 AM - 3 PM in their local timezone.\n",
    "        Weekly team meetings are mandatory on Tuesdays at 2 PM EST.\n",
    "        Home office stipend: $500 annually for equipment.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"type\": \"policy\", \"category\": \"remote_work\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "        IT Security Requirements for Remote Workers\n",
    "        \n",
    "        1. Use company-approved VPN at all times\n",
    "        2. Enable 2FA on all corporate accounts\n",
    "        3. Encrypt all devices with BitLocker or FileVault\n",
    "        4. Never share credentials via email or chat\n",
    "        5. Report suspicious emails to security@company.com\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"type\": \"policy\", \"category\": \"security\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "response = call_latent_mas(\n",
    "    prompt=\"What are the remote work policies and security requirements for our company?\",\n",
    "    rag_documents=custom_docs,\n",
    "    no_default_data=True,  # Only use provided documents\n",
    "    max_tokens=600,\n",
    "    temperature=0.6\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28746e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 10: RAG with medical knowledge (using built-in data)\n",
    "print(\"üîπ Example 10: RAG with Built-in Medical Data\")\n",
    "\n",
    "response = call_latent_mas(\n",
    "    prompt=\"What medications are commonly prescribed for hypertension and what are their mechanisms of action?\",\n",
    "    no_default_data=False,  # Use built-in medical knowledge from data/\n",
    "    max_tokens=700,\n",
    "    temperature=0.6\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5873fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 11: RAG with cryptocurrency data\n",
    "print(\"üîπ Example 11: RAG with Cryptocurrency Data\")\n",
    "\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Analyze the Bitcoin price trends and provide insights on market volatility.\",\n",
    "    no_default_data=False,  # Uses built-in CryptocurrencyData.csv\n",
    "    max_tokens=600,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd1bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 12: RAG with JSON data (base64 encoded)\n",
    "print(\"üîπ Example 12: RAG with Base64 Encoded JSON\")\n",
    "\n",
    "# Create a JSON dataset\n",
    "import json\n",
    "import base64\n",
    "\n",
    "product_data = {\n",
    "    \"products\": [\n",
    "        {\n",
    "            \"id\": \"P001\",\n",
    "            \"name\": \"Wireless Headphones\",\n",
    "            \"price\": 79.99,\n",
    "            \"features\": [\"Bluetooth 5.0\", \"40h battery\", \"Noise cancellation\"],\n",
    "            \"rating\": 4.5\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"P002\",\n",
    "            \"name\": \"Smart Watch\",\n",
    "            \"price\": 199.99,\n",
    "            \"features\": [\"Heart rate monitor\", \"GPS\", \"Water resistant\"],\n",
    "            \"rating\": 4.7\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"P003\",\n",
    "            \"name\": \"USB-C Hub\",\n",
    "            \"price\": 49.99,\n",
    "            \"features\": [\"7 ports\", \"4K HDMI\", \"Fast charging\"],\n",
    "            \"rating\": 4.3\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Encode to base64\n",
    "json_str = json.dumps(product_data)\n",
    "rag_data_b64 = base64.b64encode(json_str.encode()).decode()\n",
    "\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Compare the smart watch and wireless headphones. Which one offers better value for money?\",\n",
    "    rag_data=rag_data_b64,\n",
    "    no_default_data=True,\n",
    "    max_tokens=500,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be5c78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Conversation Continuity\n",
    "\n",
    "Maintain context across multiple requests using session and conversation IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 13: Multi-turn conversation\n",
    "print(\"üîπ Example 13: Multi-turn Conversation\")\n",
    "\n",
    "# First message - create new session\n",
    "response1 = call_latent_mas(\n",
    "    prompt=\"I'm planning a trip to Japan. Can you suggest some must-visit cities?\",\n",
    "    session_id=\"user-123-travel-planning\",\n",
    "    max_tokens=400,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response1)\n",
    "\n",
    "# Extract conversation ID for continuity\n",
    "conversation_id = response1.get(\"conversation_id\")\n",
    "session_id = response1.get(\"session_id\")\n",
    "\n",
    "print(f\"\\nüìù Conversation ID saved: {conversation_id[:16]}...\")\n",
    "print(f\"üìù Session ID saved: {session_id[:16]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f4c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation from previous cell\n",
    "print(\"üîπ Follow-up: Continue Conversation\")\n",
    "\n",
    "response2 = call_latent_mas(\n",
    "    prompt=\"How long should I plan to stay in each city? I have 2 weeks total.\",\n",
    "    conversation_id=conversation_id,\n",
    "    session_id=session_id,\n",
    "    max_tokens=400,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc952d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another follow-up\n",
    "print(\"üîπ Follow-up: Ask about specific details\")\n",
    "\n",
    "response3 = call_latent_mas(\n",
    "    prompt=\"What's the best time of year to visit? I want to see cherry blossoms.\",\n",
    "    conversation_id=conversation_id,\n",
    "    session_id=session_id,\n",
    "    max_tokens=400,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3641154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 14: Multiple conversations in one session\n",
    "print(\"üîπ Example 14: Multiple Conversations in Same Session\")\n",
    "\n",
    "# Start a new conversation in the same session\n",
    "response4 = call_latent_mas(\n",
    "    prompt=\"Now let's talk about Japanese cuisine. What dishes should I try?\",\n",
    "    session_id=session_id,  # Same session, but no conversation_id = new conversation\n",
    "    max_tokens=400,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response4)\n",
    "\n",
    "new_conversation_id = response4.get(\"conversation_id\")\n",
    "print(f\"\\nüìù New conversation started: {new_conversation_id[:16]}...\")\n",
    "print(f\"   (still in session: {session_id[:16]}...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f9aaf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. LoRA Adapter Selection\n",
    "\n",
    "The system supports multiple domain-specific LoRA adapters. You can list available adapters and select specific ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 15: List available LoRA adapters\n",
    "print(\"üîπ Example 15: List Available LoRA Adapters\")\n",
    "\n",
    "loras = list_available_loras()\n",
    "\n",
    "print(\"\\nüìö Available LoRA Adapters:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if \"loras\" in loras:\n",
    "    for name, info in loras[\"loras\"].items():\n",
    "        print(f\"\\nüî∏ {name}\")\n",
    "        print(f\"   Domain: {info.get('domain', 'N/A')}\")\n",
    "        print(f\"   Description: {info.get('description', 'N/A')}\")\n",
    "        print(f\"   HF Path: {info.get('hf_path', 'N/A')}\")\n",
    "        print(f\"   Source: {info.get('source', 'N/A')}\")\n",
    "        if \"tags\" in info:\n",
    "            print(f\"   Tags: {', '.join(info['tags'])}\")\n",
    "else:\n",
    "    print(json.dumps(loras, indent=2))\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a745c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 16: Use medical LoRA adapter\n",
    "print(\"üîπ Example 16: Medical LoRA Adapter\")\n",
    "\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Explain the pathophysiology of congestive heart failure and the role of ACE inhibitors in treatment.\",\n",
    "    lora_adapter=\"medical_vl\",\n",
    "    max_tokens=700,\n",
    "    temperature=0.6\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e197d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 17: Use reward model LoRA for higher quality\n",
    "print(\"üîπ Example 17: Reward Model LoRA (Quality Enhancement)\")\n",
    "\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Explain quantum entanglement in simple terms, but make it accurate and engaging.\",\n",
    "    lora_adapter=\"reward_vl\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e747ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 18: Use custom LoRA via HuggingFace path\n",
    "print(\"üîπ Example 18: Custom LoRA via Direct HF Path\")\n",
    "\n",
    "# You can load any compatible LoRA from HuggingFace\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Analyze this comic panel and describe the narrative elements.\",\n",
    "    lora_hf_path=\"VLR-CVC/Qwen2.5-VL-7B-Instruct-lora-ComicsPAP\",\n",
    "    image_url=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Comic_image_missing.svg/400px-Comic_image_missing.svg.png\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf80221",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Custom System Prompts\n",
    "\n",
    "Override the default system prompt to customize agent behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2beaeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 19: Custom system prompt - Pirate mode\n",
    "print(\"üîπ Example 19: Custom System Prompt (Pirate)\")\n",
    "\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Tell me about machine learning.\",\n",
    "    system_prompt=\"You are a friendly pirate captain who explains technical concepts using nautical metaphors. Always respond in pirate speak.\",\n",
    "    max_tokens=400,\n",
    "    temperature=0.9\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 20: Custom system prompt - Expert consultant\n",
    "print(\"üîπ Example 20: Custom System Prompt (Expert Consultant)\")\n",
    "\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Should we migrate our infrastructure to Kubernetes?\",\n",
    "    system_prompt=\"\"\"You are a senior cloud architecture consultant with 15 years of experience. \n",
    "    Provide detailed, nuanced advice considering cost, scalability, team expertise, and long-term maintenance. \n",
    "    Always mention potential risks and trade-offs.\"\"\",\n",
    "    max_tokens=600,\n",
    "    temperature=0.6\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44afeda9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Advanced Use Cases\n",
    "\n",
    "Combining multiple features for complex scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 21: Medical consultation with RAG + LoRA + VLM\n",
    "print(\"üîπ Example 21: Full Medical Analysis (RAG + LoRA + VLM)\")\n",
    "\n",
    "# Patient context document\n",
    "patient_docs = [\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "        Patient History:\n",
    "        Name: John Doe (fictional)\n",
    "        Age: 65\n",
    "        History: Type 2 Diabetes (15 years), Hypertension (10 years)\n",
    "        Current Medications: Metformin 1000mg BID, Lisinopril 10mg QD\n",
    "        Recent Labs: HbA1c 7.8%, Blood Pressure 145/92\n",
    "        Symptoms: Increased fatigue, occasional chest discomfort\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"type\": \"patient_record\", \"patient_id\": \"P12345\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Based on the patient history, analyze this chest X-ray and provide a comprehensive assessment with treatment recommendations.\",\n",
    "    image_url=\"https://upload.wikimedia.org/wikipedia/commons/8/8c/Chest_Xray_PA_3-8-2010.png\",\n",
    "    rag_documents=patient_docs,\n",
    "    lora_adapter=\"medical_vl\",\n",
    "    no_default_data=False,  # Include medical knowledge base\n",
    "    max_tokens=800,\n",
    "    temperature=0.6,\n",
    "    system_prompt=\"You are an experienced physician providing detailed medical analysis. Always consider patient history and current medications.\"\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44257b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 22: Research assistant with conversation history\n",
    "print(\"üîπ Example 22: Research Assistant with Context\")\n",
    "\n",
    "# Start research session\n",
    "research_session = \"research-session-ai-ethics\"\n",
    "\n",
    "# First query\n",
    "response1 = call_latent_mas(\n",
    "    prompt=\"What are the main ethical concerns regarding AI in healthcare?\",\n",
    "    session_id=research_session,\n",
    "    max_tokens=600,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response1)\n",
    "\n",
    "research_conv_id = response1.get(\"conversation_id\")\n",
    "\n",
    "# Follow-up with context\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí¨ Follow-up Query\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "response2 = call_latent_mas(\n",
    "    prompt=\"Can you elaborate on the privacy concerns you mentioned?\",\n",
    "    conversation_id=research_conv_id,\n",
    "    session_id=research_session,\n",
    "    max_tokens=500,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response2)\n",
    "\n",
    "# Add research documents\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí¨ Adding Research Context\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "research_docs = [\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "        Recent Study: AI in Medical Diagnosis\n",
    "        Published: 2026\n",
    "        \n",
    "        Key Findings:\n",
    "        - AI models achieve 94% accuracy in radiology diagnoses\n",
    "        - 23% reduction in diagnostic errors when AI assists physicians\n",
    "        - Patient privacy concerns: 67% of patients worried about data sharing\n",
    "        - Algorithmic bias detected in 31% of skin condition diagnoses\n",
    "        - Healthcare costs reduced by 18% with AI triage systems\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"type\": \"research\", \"year\": 2026}\n",
    "    }\n",
    "]\n",
    "\n",
    "response3 = call_latent_mas(\n",
    "    prompt=\"Based on this recent research, what solutions would you propose for the privacy and bias issues?\",\n",
    "    conversation_id=research_conv_id,\n",
    "    session_id=research_session,\n",
    "    rag_documents=research_docs,\n",
    "    max_tokens=700,\n",
    "    temperature=0.7\n",
    ")\n",
    "print_response(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb852c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 23: Code review with technical context\n",
    "print(\"üîπ Example 23: Code Review Session\")\n",
    "\n",
    "code_context = [\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "        # Current Implementation\n",
    "        def process_user_data(data):\n",
    "            results = []\n",
    "            for item in data:\n",
    "                if item['status'] == 'active':\n",
    "                    results.append({\n",
    "                        'id': item['id'],\n",
    "                        'name': item['name'],\n",
    "                        'email': item['email'],\n",
    "                        'score': item['score'] * 1.5\n",
    "                    })\n",
    "            return results\n",
    "        \n",
    "        # Issues:\n",
    "        # - No input validation\n",
    "        # - Inefficient list concatenation\n",
    "        # - Magic number (1.5)\n",
    "        # - No error handling\n",
    "        # - Direct dict access without checks\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"type\": \"code\", \"language\": \"python\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "response = call_latent_mas(\n",
    "    prompt=\"Review this code and suggest improvements focusing on performance, security, and maintainability. Provide a refactored version.\",\n",
    "    rag_documents=code_context,\n",
    "    no_default_data=True,\n",
    "    max_tokens=800,\n",
    "    temperature=0.5,\n",
    "    system_prompt=\"You are a senior Python developer conducting a thorough code review. Focus on best practices, security, and performance.\"\n",
    ")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc04622",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Session & Conversation Management\n",
    "\n",
    "List and manage your conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836bd881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 24: List all conversations\n",
    "print(\"üîπ Example 24: List All Conversations\")\n",
    "\n",
    "conversations = list_conversations()\n",
    "\n",
    "print(\"\\nüìö Active Sessions & Conversations:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if \"sessions\" in conversations:\n",
    "    for session_id, session_info in conversations[\"sessions\"].items():\n",
    "        print(f\"\\nüìÅ Session: {session_id[:16]}...\")\n",
    "        print(f\"   Created: {session_info.get('created', 'N/A')}\")\n",
    "        print(f\"   Conversations: {len(session_info.get('conversations', []))}\")\n",
    "        \n",
    "        for conv_id in session_info.get(\"conversations\", []):\n",
    "            print(f\"      üí¨ {conv_id[:16]}...\")\n",
    "else:\n",
    "    print(json.dumps(conversations, indent=2))\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a9c51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Temperature & Creativity Control\n",
    "\n",
    "Experiment with different temperature settings for various use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 25: Low temperature (factual, deterministic)\n",
    "print(\"üîπ Example 25: Low Temperature (0.3) - Factual Response\")\n",
    "\n",
    "response_low = call_latent_mas(\n",
    "    prompt=\"What is the boiling point of water at sea level?\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.3\n",
    ")\n",
    "print_response(response_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 26: High temperature (creative, diverse)\n",
    "print(\"üîπ Example 26: High Temperature (1.0) - Creative Response\")\n",
    "\n",
    "response_high = call_latent_mas(\n",
    "    prompt=\"Write a short story about a robot learning to paint.\",\n",
    "    max_tokens=500,\n",
    "    temperature=1.0\n",
    ")\n",
    "print_response(response_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25992898",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Batch Processing Example\n",
    "\n",
    "Process multiple queries efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 27: Batch processing\n",
    "print(\"üîπ Example 27: Batch Processing Multiple Queries\")\n",
    "\n",
    "queries = [\n",
    "    {\"prompt\": \"What is machine learning?\", \"domain\": \"general\"},\n",
    "    {\"prompt\": \"Explain gradient descent algorithm\", \"domain\": \"math\"},\n",
    "    {\"prompt\": \"Write a Python decorator for timing functions\", \"domain\": \"code\"},\n",
    "    {\"prompt\": \"What are the symptoms of pneumonia?\", \"domain\": \"medical\"},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"\\nüîÑ Processing {len(queries)} queries...\\n\")\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"[{i}/{len(queries)}] Processing: {query['prompt'][:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        response = call_latent_mas(\n",
    "            prompt=query[\"prompt\"],\n",
    "            max_tokens=300,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        results.append({\n",
    "            \"query\": query[\"prompt\"],\n",
    "            \"response\": response.get(\"response\", \"\"),\n",
    "            \"domain\": response.get(\"domain\", \"\"),\n",
    "            \"confidence\": response.get(\"domain_confidence\", 0)\n",
    "        })\n",
    "        print(f\"    ‚úÖ Completed (Domain: {response.get('domain', 'N/A')})\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå Failed: {e}\")\n",
    "        results.append({\n",
    "            \"query\": query[\"prompt\"],\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting\n",
    "\n",
    "print(f\"\\n‚úÖ Batch processing complete: {len(results)} results\\n\")\n",
    "\n",
    "# Display summary\n",
    "print(\"=\"*60)\n",
    "print(\"üìä BATCH RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Query: {result['query'][:60]}...\")\n",
    "    if \"error\" in result:\n",
    "        print(f\"   ‚ùå Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"   Domain: {result['domain']} (confidence: {result['confidence']:.2f})\")\n",
    "        print(f\"   Response: {result['response'][:100]}...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73fd7d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Error Handling & Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 28: Robust error handling\n",
    "print(\"üîπ Example 28: Error Handling Best Practices\")\n",
    "\n",
    "def safe_query(prompt: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Safely execute a query with comprehensive error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = call_latent_mas(\n",
    "            prompt=prompt,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        if \"error\" in response:\n",
    "            print(f\"‚ùå API Error: {response['error']}\")\n",
    "            return None\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"‚ùå Request timed out. The model may be cold-starting or overloaded.\")\n",
    "        return None\n",
    "        \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"‚ùå HTTP Error: {e}\")\n",
    "        if e.response.status_code == 401:\n",
    "            print(\"   Check your API key\")\n",
    "        elif e.response.status_code == 404:\n",
    "            print(\"   Check your endpoint ID\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with error handling\n",
    "response = safe_query(\n",
    "    \"What are the benefits of yoga?\",\n",
    "    max_tokens=400,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "if response:\n",
    "    print_response(response)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Query failed, see error above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6752b1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Performance Tips & Best Practices\n",
    "\n",
    "### Optimization Strategies:\n",
    "\n",
    "1. **Reuse Sessions**: Keep `session_id` for multi-turn conversations to maintain context\n",
    "2. **Temperature Control**:\n",
    "   - Low (0.3-0.5): Factual queries, code generation, medical info\n",
    "   - Medium (0.6-0.8): General chat, explanations\n",
    "   - High (0.9-1.0): Creative writing, brainstorming\n",
    "3. **RAG Optimization**:\n",
    "   - Use `no_default_data=True` if you don't need built-in knowledge\n",
    "   - Provide focused, relevant documents (not entire websites)\n",
    "   - Limit to 5-10 documents per query\n",
    "4. **LoRA Selection**:\n",
    "   - Use domain-specific adapters when available\n",
    "   - Medical: `medical_vl`\n",
    "   - Quality: `reward_vl`\n",
    "   - Comics/Visual: `comics_vl`\n",
    "5. **Token Management**:\n",
    "   - Start with `max_tokens=200-400` for quick responses\n",
    "   - Increase to 600-800 for detailed explanations\n",
    "   - Monitor costs (longer = more expensive)\n",
    "6. **Rate Limiting**:\n",
    "   - Add delays between batch requests\n",
    "   - RunPod may have per-endpoint rate limits\n",
    "7. **Cold Start**:\n",
    "   - First request may take 30-60s (model loading)\n",
    "   - Subsequent requests are fast (< 5s)\n",
    "\n",
    "### Common Pitfalls:\n",
    "\n",
    "‚ùå **Don't** send huge documents (>10MB) via RAG\n",
    "‚úÖ **Do** chunk and summarize first\n",
    "\n",
    "‚ùå **Don't** use high temperature for factual queries\n",
    "‚úÖ **Do** use 0.3-0.5 for accuracy\n",
    "\n",
    "‚ùå **Don't** create new sessions for every message\n",
    "‚úÖ **Do** reuse `session_id` for related conversations\n",
    "\n",
    "‚ùå **Don't** ignore domain routing results\n",
    "‚úÖ **Do** use domain info to select appropriate LoRAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba413c46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. API Reference Summary\n",
    "\n",
    "### Core Parameters\n",
    "\n",
    "| Parameter | Type | Required | Default | Description |\n",
    "|-----------|------|----------|---------|-------------|\n",
    "| `prompt` | string | ‚úÖ Yes | - | The query text |\n",
    "| `max_tokens` | int | No | 800 | Maximum tokens to generate |\n",
    "| `temperature` | float | No | 0.7 | Sampling temperature (0.0-1.0) |\n",
    "| `model` | string | No | Qwen/Qwen2.5-VL-7B-Instruct | Model name |\n",
    "\n",
    "### Vision (VLM) Parameters\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `image_url` | string | No | URL of an image for analysis |\n",
    "| `image_base64` | string | No | Base64-encoded image data |\n",
    "\n",
    "### RAG Parameters\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `rag_documents` | list[dict] | No | Inline documents for context |\n",
    "| `rag_data` | string | No | URL or base64 JSON/CSV data |\n",
    "| `no_default_data` | bool | No | Skip built-in knowledge base |\n",
    "\n",
    "### Conversation Parameters\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `session_id` | string | No | Session ID for grouping conversations |\n",
    "| `conversation_id` | string | No | Continue existing conversation |\n",
    "| `system_prompt` | string | No | Custom system prompt |\n",
    "\n",
    "### LoRA Parameters\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `lora_adapter` | string | No | Adapter name from registry |\n",
    "| `lora_hf_path` | string | No | Direct HuggingFace LoRA path |\n",
    "\n",
    "### Special Requests\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `list_loras` | bool | Return available LoRA adapters |\n",
    "| `list_conversations` | bool | Return saved sessions |\n",
    "\n",
    "### Response Fields\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `response` | string | The generated answer |\n",
    "| `conversation_id` | string | Conversation ID (for continuity) |\n",
    "| `session_id` | string | Session ID |\n",
    "| `domain` | string | Detected domain (medical, code, math, etc.) |\n",
    "| `domain_confidence` | float | Domain classification confidence |\n",
    "| `model` | string | Model used |\n",
    "| `vlm` | bool | VLM mode active |\n",
    "| `image_provided` | bool | Image was provided |\n",
    "| `rag_enabled` | bool | RAG is active |\n",
    "| `lora` | dict | LoRA adapter info |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81835671",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15. Conclusion & Next Steps\n",
    "\n",
    "This notebook demonstrates all major features of the **LatentMAS + S-LoRA** system via the RunPod API:\n",
    "\n",
    "### ‚úÖ What We Covered:\n",
    "1. ‚ú® **Basic chat** with multi-agent reasoning\n",
    "2. üñºÔ∏è **Vision Language Model** for image analysis\n",
    "3. üìö **RAG** for document-aware responses\n",
    "4. üí¨ **Conversation continuity** with sessions\n",
    "5. üîß **LoRA adapters** for domain specialization\n",
    "6. üéõÔ∏è **Custom system prompts** for behavior control\n",
    "7. üîÑ **Batch processing** for efficiency\n",
    "8. üõ°Ô∏è **Error handling** best practices\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "- Share this notebook with your GitHub team\n",
    "- Experiment with different LoRA adapters for your use case\n",
    "- Build custom RAG pipelines with your domain data\n",
    "- Integrate into your production applications\n",
    "- Monitor costs and optimize token usage\n",
    "\n",
    "### üìö Resources:\n",
    "- **GitHub**: [latent_mas_slora](https://github.com/Arifuzzamanjoy/latent_mas_slora)\n",
    "- **Docs**: See `docs/` folder in repo\n",
    "- **Docker Image**: `s1710374103/latent-mas-slora:latest`\n",
    "- **RunPod**: https://runpod.io\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or Issues?**\n",
    "- Check the [README.md](../README.md)\n",
    "- Review [ARCHITECTURE.md](../docs/ARCHITECTURE.md)\n",
    "- Open an issue on GitHub\n",
    "\n",
    "**Happy experimenting! üéâ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
